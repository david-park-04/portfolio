Naive Bayes is a classification algorithm that utilizes Bayes' Rules and assumes conditional independence to reduce the number of variables.

Conditional independence indicates that given some event that occurs, the probability of A and B for $P(A, B | C)$ are separate.

# Application
Let's consider a classifier that sorts a fish as either a salmon or a sea bass according to its length and color. So, the features are length (long or short) and color (silvery or grey), and the labels are salmon and sea bass.

If we could not assume conditional independence, we would have to compute all possible feature combinations. Instead, we can take the features individually and take their probabilities given a label.

# Other Applications
When features take on values from continuous space, we assume that each feature follows a Gaussian distribution and calculate the probability with a probability density function.

When dealing with zeros, where there is no instance of an event, we apply Laplace smoothing.
Laplace smoothing pretends we have an extra occurrence of each value of the feature for every class.