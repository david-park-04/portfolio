In the context of machine learning, probability allows for reasoning about uncertainty, which is inevitable in the real world.

# Basic Concepts
- Experiment: The process by which an observation is obtained
- Event: An outcome of an experiment; a collection of one or more simple events - the probability of an event can be calculated through the sum of simple events
	- Simple event: An event that cannot be decomposed
- Sample space: The set of all simple events
- Mutually exclusive: Two events are mutually exclusive if 

# Counting Rules
Counting rules help us determine the size of the sample space.

## The mn rule
For an experiment with two stages, with m possibilities at the first stage and n possibilities at the second stage, the total amount of possibilities is $m \cdot n$.

This rule can be generalized for an arbitrary number of stages by continuing to multiply possibilities: $n_1 \cdot n_2 \cdot n_3...$ 

# Relationships Between Events
Events can be combined together to form other events. To calculate the probabilities for these events, there are some equations to keep in mind.

- Union: $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
- Complement: $P(A^c) = 1 - P(A)$
- Intersections
	- Conditional probabilities
		$P(A | B) = \frac{P(A \cap B)}{P(B)}$

		- For independent events, $P(A)$ is the same

	- The multiplicative rule for intersections
		- Independent events: $P(A \cap B) = P(A)P(B)$ 
		- Dependent events: $P(A \cap B) = P(A)P(B | A)$ 
		
	- Bayes' Rule
		$P(S_i | A) = \frac{P(S_i)P(A | S_i)}{\sum P(S_i)P(A | S_i)}$, given that $S_i$ are mutually exclusive and exhaustive events

		- Allows you to update probabilities given new evidence